{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Personal notes about engineering Objective Find quickly often-used concepts/formulas/algorithms/code Explain these concepts in my own words Help me remember these concepts when I build these notes Guidelines Be efficient Only pursue what is approachable and interesting. Do it as you can, no perfection. Store knowledge, not information Do not be thorough but concise, only what matters along with the though path I need to understand it Write in my own words to myself TODO Topic Details Status Algebra basics Matrix inv, inner prod,... Stand-by Kalman Proof, link to Luenberger In Progress ROS2 For Kalman demo Stand-by Python basics, numPy, SciPy, OpenCV In Progress C++ basics, Eigen, OpenCV Stand-by Optimization DIRECT,... Not Started Information theory Not Started In Progess Camera calibration Knowing distance between red and yellow dot, compute distance, with a few assumptions (in vertical plane no z-rotation w.r.t. camera) Compute measurement angle Implement Kalman filter Improve rviz for real-time demo Check and tune rates","title":"Home"},{"location":"#personal-notes-about-engineering","text":"","title":"Personal notes about engineering"},{"location":"#objective","text":"Find quickly often-used concepts/formulas/algorithms/code Explain these concepts in my own words Help me remember these concepts when I build these notes","title":"Objective"},{"location":"#guidelines","text":"Be efficient Only pursue what is approachable and interesting. Do it as you can, no perfection. Store knowledge, not information Do not be thorough but concise, only what matters along with the though path I need to understand it Write in my own words to myself","title":"Guidelines"},{"location":"#todo","text":"Topic Details Status Algebra basics Matrix inv, inner prod,... Stand-by Kalman Proof, link to Luenberger In Progress ROS2 For Kalman demo Stand-by Python basics, numPy, SciPy, OpenCV In Progress C++ basics, Eigen, OpenCV Stand-by Optimization DIRECT,... Not Started Information theory Not Started","title":"TODO"},{"location":"#in-progess","text":"Camera calibration Knowing distance between red and yellow dot, compute distance, with a few assumptions (in vertical plane no z-rotation w.r.t. camera) Compute measurement angle Implement Kalman filter Improve rviz for real-time demo Check and tune rates","title":"In Progess"},{"location":"Algebra/matrix_inversion/","text":"Matrix Inversion Inverse of matrix A is A^{-1} such that AA^{-1}=A^{-1}A=I . Adjugate Method Inverse of A using the adjugate method: A^{-1} = \\frac{1}{\\det(A)} \\text{adj}(A) \\text{adj}(A) = C^T with C the cofactor matrix, whose element c_{i,j} is (-1)^{i+j} times the determinant of A without the i -th line and j -th column. Example for n=2 A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} A^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix} Example for n=3 A = \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{pmatrix} C = \\begin{pmatrix} ei-fh & -(di-fg) & dh-eg \\\\ -(bi-ch) & ai-cg & -(ah-bg) \\\\ bf-ce & -(af-cd) & ae-bd \\end{pmatrix} A^{-1} = \\frac{1}{\\det(A)} C^T","title":"matrix inversion"},{"location":"Algebra/matrix_inversion/#matrix-inversion","text":"Inverse of matrix A is A^{-1} such that AA^{-1}=A^{-1}A=I .","title":"Matrix Inversion"},{"location":"Algebra/matrix_inversion/#adjugate-method","text":"Inverse of A using the adjugate method: A^{-1} = \\frac{1}{\\det(A)} \\text{adj}(A) \\text{adj}(A) = C^T with C the cofactor matrix, whose element c_{i,j} is (-1)^{i+j} times the determinant of A without the i -th line and j -th column.","title":"Adjugate Method"},{"location":"Algebra/matrix_inversion/#example-for-n2","text":"A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} A^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}","title":"Example for n=2"},{"location":"Algebra/matrix_inversion/#example-for-n3","text":"A = \\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{pmatrix} C = \\begin{pmatrix} ei-fh & -(di-fg) & dh-eg \\\\ -(bi-ch) & ai-cg & -(ah-bg) \\\\ bf-ce & -(af-cd) & ae-bd \\end{pmatrix} A^{-1} = \\frac{1}{\\det(A)} C^T","title":"Example for n=3"},{"location":"Algebra/solve_linear_eq_Cramer_rule/","text":"","title":"linear system"},{"location":"Estimation/Bayesian_inference/","text":"Bayesian inference Bayes' theorem Obtained from the relation between conditional and joint probabilites. Events \\(A\\) and \\(B\\) $$ \\mathbb{P}(A|B) \\mathbb{P}(B) = \\mathbb{P}(A,B) = \\mathbb{P}(B|A) \\mathbb{P}(A) $$ $$ \\mathbb{P}(A|B) = \\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(A)} $$ Discrete random variables \\(X\\) and \\(Y\\) $$ p(X=x|Y=y) p(Y=y) = p(\\lbrace X=x\\rbrace ,\\lbrace Y=y\\rbrace) = p(Y=y|X=x) p(X=x) $$ $$ p(X=x|Y=y) = \\frac{p(Y=y|X=x) p(X=x)}{p(Y=y)} $$ Continuous random variables \\(X\\) and \\(Y\\) $$ p_{X|Y}(x|y) p_Y(y) = p_{X,Y}(x,y) = p_{Y|X}(y|x) p_X(x) $$ Omitting the subscript notation for the random variable, $$ p(x|y) p(y) = p(x,y) = p(y|x) p(x) $$ Bayes' theorem is $$ p(x|y) = \\frac{p(y|x) p(x)}{p(y)} $$ The rest of this note only deals with continuous random variables. Recursive Bayesian inference for parameter estimation With prior distribution p(\\theta) , measurement model p(y|\\theta) and measurements y_{1:N} , the estimation for model parameter \\theta is p(\\theta|y_{1:N}) = \\frac{p(y_{1:N}|\\theta) p(\\theta)}{p(y_{1:N})} As measurements are typically received successively, the posterior of one instant becomes the prior of the next. \\begin{aligned} p(\\theta|y_1) &= \\frac{p(y_1|\\theta)p(\\theta)}{p(y_1)} \\\\ p(\\theta|y_{1:2}) &= \\frac{p(y_2|\\theta)p(\\theta|y_1)}{p(y_2)} \\\\ &\\ldots \\end{aligned} Thus we are interested in p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} The denominator is only a normalizing constant so the PDF integrates to 1. We can either compute the denominator, by integrating out over \\theta , p(y) = \\int p(y|\\theta)p(\\theta)d\\theta discard the denominator, if working with a quantity only proportionnal to p(\\theta|y) is enough, p(\\theta|y) \\propto p(y|\\theta) p(\\theta) Example for \\(y=\\theta + \\epsilon\\) with measurement noise \\(\\epsilon=\\mathcal{N}(0,\\sigma_y^2)\\) and a priori \\(\\theta\\sim\\mathcal{N}(\\mu_0,\\sigma_0^2)\\), the posterior is proportional to a product of gaussians, itself proportional to another gaussian, $$ \\begin{aligned} p(\\theta|y) &\\propto p(y|\\theta)p(\\theta) \\\\ &\\propto \\mathcal{N}(\\mu_1,\\sigma_1^2) \\end{aligned} $$ $$ \\begin{aligned} \\mu_1 &= \\frac{1}{\\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_0^2}} \\left(\\frac{y}{\\sigma_y^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) \\\\ \\sigma_1^2 &= \\frac{1}{\\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_0^2}} \\end{aligned} $$ $$ \\begin{aligned} p(\\theta|y) &\\propto \\exp\\left(-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma_y^2}\\right) \\exp\\left(-\\frac{1}{2}\\frac{(\\theta-\\mu_0)^2}{\\sigma_0^2}\\right) \\\\ &=\\exp\\left(-\\frac{1}{2}\\left(\\frac{y^2-2y\\theta+\\theta^2}{\\sigma_y^2}+\\frac{\\theta^2-2\\theta\\mu_0+\\mu_0^2}{\\sigma_0^2}\\right)\\right) \\\\ &= \\exp\\left(-\\frac{1}{2}\\left(\\theta^2\\left(\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}\\right)-2\\theta\\left(\\frac{y}{\\sigma_y^2}+\\frac{\\mu_0}{\\sigma_0^2}\\right) + \\text{cst}\\right)\\right) \\end{aligned} $$ As we are interested in a quantity only proportionnal to \\(p(\\theta|y)\\), we do not multiplying the exponential by a constant, or equivalently, adding a constant in the exponential as \\(e^{f(\\theta)+a}=Ae^{f(\\theta)}\\). Hence the square can be completed with no regards for its last term, $$ \\begin{aligned} &\\propto \\exp\\left(-\\frac{1}{2}\\left(\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}\\right)\\left(\\theta - \\frac{1}{\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}}\\left(\\frac{y}{\\sigma_y^2}+\\frac{\\mu_0}{\\sigma_0^2}\\right)\\right)^2\\right) \\\\ &= \\exp\\left(-\\frac{1}{2}\\frac{\\left(\\theta - \\frac{1}{\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}}\\left(\\frac{y}{\\sigma_y^2}+\\frac{\\mu_0}{\\sigma_0^2}\\right)\\right)^2}{\\frac{1}{\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}}}\\right) \\end{aligned} $$ For the multivariate case, \\(p(\\theta|y) \\propto \\mathcal{N}(\\mu_1,\\Sigma_1)\\) with $$ \\begin{aligned} \\mu_1 &=\\left[\\Sigma_y^{-1} + \\Sigma_0^{-1}\\right]^{-1} \\left[\\Sigma_y^{-1}y + \\Sigma_0^{-1}\\mu_0\\right] \\\\ \\Sigma_1 &= \\left[\\Sigma_y^{-1} + \\Sigma_0^{-1}\\right]^{-1} \\end{aligned} $$ Example for \\(y=\\sum\\theta_i x_i + \\epsilon = X\\theta + \\epsilon \\) with input vector \\(X=(x_1,\\ldots,x_n)\\), measurement noise \\(\\epsilon=\\mathcal{N}(0,\\sigma_y^2)\\) and a priori \\(\\theta\\sim\\mathcal{N}(\\mu_0,\\Sigma_0)\\), $$ p(\\theta|y) \\propto \\mathcal{N}(\\mu_1,\\Sigma_1^2) $$ $$ \\begin{aligned} \\mu_1 &=\\left[\\frac{X^\\top X}{\\sigma_y^2} + \\Sigma_0^{-1}\\right]^{-1} \\left[\\frac{X^\\top y}{\\sigma_y^2} + \\Sigma_0^{-1}\\mu_0\\right] \\\\ \\Sigma_1 &= \\left[\\frac{X^\\top X}{\\sigma_y^2} + \\Sigma_0^{-1}\\right]^{-1} \\end{aligned} $$ $$ \\begin{aligned} p(\\theta|y) &\\propto \\exp\\left(-\\frac{1}{2}\\frac{(y-X\\theta)^2}{\\sigma_y^2}\\right) \\exp\\left(-\\frac{1}{2}(\\theta-\\mu_0)^\\top\\Sigma_0^{-1}(\\theta-\\mu_0)\\right) \\\\ &=\\exp\\left(-\\frac{1}{2}\\left(\\frac{y^2-2yX\\theta+(X\\theta)^2}{\\sigma_y^2}\\theta^\\top\\Sigma_0^{-1}\\theta - 2\\theta^\\top\\Sigma_0^{-1}\\mu_0 + \\mu_0^\\top\\Sigma_0^{-1}\\mu_0\\right)\\right) \\end{aligned} $$ Using \\((X\\theta)^2 = (X\\theta)^\\top X\\theta = \\theta^\\top X^\\top X \\theta\\) and \\(yX\\theta = (yX\\theta)^\\top = \\theta^\\top X^\\top y\\), $$ = \\exp\\left(-\\frac{1}{2}\\left(\\theta^\\top\\left(\\frac{X^\\top X}{\\sigma_y^2}+\\Sigma_0^{-1}\\right)\\theta - 2\\theta^\\top\\left(\\frac{X^\\top y}{\\sigma_y^2}+\\Sigma_0^{-1}\\mu_0\\right)+\\text{cst}\\right)\\right) $$ and we can directly identify the form $$ \\begin{aligned} &\\quad\\exp\\left(-\\frac{1}{2}\\left(\\theta^\\top\\Sigma_1^{-1}\\theta -2\\theta^\\top\\Sigma_1^{-1}\\mu_1 + \\ldots\\right)\\right)\\\\ &\\propto\\exp\\left(-\\frac{1}{2}\\left((\\theta-\\mu_1)^\\top\\Sigma_1^{-1}(\\theta-\\mu_1)\\right)\\right) \\end{aligned} $$ For the multivariate case with \\(y\\in\\mathbb{R}^m\\), \\(X\\in\\mathbb{R}^{m\\times n}\\), \\(\\epsilon\\sim\\mathcal{N}(0,\\Sigma_y)\\), which corresponds to mapping the different lines of \\(X\\) onto elements of \\(y\\) through the same linear transformation with coefficients \\(\\theta\\). If batches of input-output measurements are obtained, we could use this equation. $$ \\begin{aligned} \\mu_1 &=\\left[X^\\top\\Sigma_y^{-1}X + \\Sigma_0^{-1}\\right]^{-1} \\left[X^\\top\\Sigma_y^{-1}y + \\Sigma_0^{-1}\\mu_0\\right] \\\\ \\Sigma_1 &= \\left[X^\\top\\Sigma_y^{-1}X + \\Sigma_0^{-1}\\right]^{-1} \\end{aligned} $$","title":"Bayesian inference"},{"location":"Estimation/Bayesian_inference/#bayesian-inference","text":"","title":"Bayesian inference"},{"location":"Estimation/Bayesian_inference/#bayes-theorem","text":"Obtained from the relation between conditional and joint probabilites. Events \\(A\\) and \\(B\\) $$ \\mathbb{P}(A|B) \\mathbb{P}(B) = \\mathbb{P}(A,B) = \\mathbb{P}(B|A) \\mathbb{P}(A) $$ $$ \\mathbb{P}(A|B) = \\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(A)} $$ Discrete random variables \\(X\\) and \\(Y\\) $$ p(X=x|Y=y) p(Y=y) = p(\\lbrace X=x\\rbrace ,\\lbrace Y=y\\rbrace) = p(Y=y|X=x) p(X=x) $$ $$ p(X=x|Y=y) = \\frac{p(Y=y|X=x) p(X=x)}{p(Y=y)} $$ Continuous random variables \\(X\\) and \\(Y\\) $$ p_{X|Y}(x|y) p_Y(y) = p_{X,Y}(x,y) = p_{Y|X}(y|x) p_X(x) $$ Omitting the subscript notation for the random variable, $$ p(x|y) p(y) = p(x,y) = p(y|x) p(x) $$ Bayes' theorem is $$ p(x|y) = \\frac{p(y|x) p(x)}{p(y)} $$ The rest of this note only deals with continuous random variables.","title":"Bayes' theorem"},{"location":"Estimation/Bayesian_inference/#recursive-bayesian-inference-for-parameter-estimation","text":"With prior distribution p(\\theta) , measurement model p(y|\\theta) and measurements y_{1:N} , the estimation for model parameter \\theta is p(\\theta|y_{1:N}) = \\frac{p(y_{1:N}|\\theta) p(\\theta)}{p(y_{1:N})} As measurements are typically received successively, the posterior of one instant becomes the prior of the next. \\begin{aligned} p(\\theta|y_1) &= \\frac{p(y_1|\\theta)p(\\theta)}{p(y_1)} \\\\ p(\\theta|y_{1:2}) &= \\frac{p(y_2|\\theta)p(\\theta|y_1)}{p(y_2)} \\\\ &\\ldots \\end{aligned} Thus we are interested in p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)} The denominator is only a normalizing constant so the PDF integrates to 1. We can either compute the denominator, by integrating out over \\theta , p(y) = \\int p(y|\\theta)p(\\theta)d\\theta discard the denominator, if working with a quantity only proportionnal to p(\\theta|y) is enough, p(\\theta|y) \\propto p(y|\\theta) p(\\theta) Example for \\(y=\\theta + \\epsilon\\) with measurement noise \\(\\epsilon=\\mathcal{N}(0,\\sigma_y^2)\\) and a priori \\(\\theta\\sim\\mathcal{N}(\\mu_0,\\sigma_0^2)\\), the posterior is proportional to a product of gaussians, itself proportional to another gaussian, $$ \\begin{aligned} p(\\theta|y) &\\propto p(y|\\theta)p(\\theta) \\\\ &\\propto \\mathcal{N}(\\mu_1,\\sigma_1^2) \\end{aligned} $$ $$ \\begin{aligned} \\mu_1 &= \\frac{1}{\\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_0^2}} \\left(\\frac{y}{\\sigma_y^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) \\\\ \\sigma_1^2 &= \\frac{1}{\\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_0^2}} \\end{aligned} $$ $$ \\begin{aligned} p(\\theta|y) &\\propto \\exp\\left(-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma_y^2}\\right) \\exp\\left(-\\frac{1}{2}\\frac{(\\theta-\\mu_0)^2}{\\sigma_0^2}\\right) \\\\ &=\\exp\\left(-\\frac{1}{2}\\left(\\frac{y^2-2y\\theta+\\theta^2}{\\sigma_y^2}+\\frac{\\theta^2-2\\theta\\mu_0+\\mu_0^2}{\\sigma_0^2}\\right)\\right) \\\\ &= \\exp\\left(-\\frac{1}{2}\\left(\\theta^2\\left(\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}\\right)-2\\theta\\left(\\frac{y}{\\sigma_y^2}+\\frac{\\mu_0}{\\sigma_0^2}\\right) + \\text{cst}\\right)\\right) \\end{aligned} $$ As we are interested in a quantity only proportionnal to \\(p(\\theta|y)\\), we do not multiplying the exponential by a constant, or equivalently, adding a constant in the exponential as \\(e^{f(\\theta)+a}=Ae^{f(\\theta)}\\). Hence the square can be completed with no regards for its last term, $$ \\begin{aligned} &\\propto \\exp\\left(-\\frac{1}{2}\\left(\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}\\right)\\left(\\theta - \\frac{1}{\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}}\\left(\\frac{y}{\\sigma_y^2}+\\frac{\\mu_0}{\\sigma_0^2}\\right)\\right)^2\\right) \\\\ &= \\exp\\left(-\\frac{1}{2}\\frac{\\left(\\theta - \\frac{1}{\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}}\\left(\\frac{y}{\\sigma_y^2}+\\frac{\\mu_0}{\\sigma_0^2}\\right)\\right)^2}{\\frac{1}{\\frac{1}{\\sigma_y^2}+\\frac{1}{\\sigma_0^2}}}\\right) \\end{aligned} $$ For the multivariate case, \\(p(\\theta|y) \\propto \\mathcal{N}(\\mu_1,\\Sigma_1)\\) with $$ \\begin{aligned} \\mu_1 &=\\left[\\Sigma_y^{-1} + \\Sigma_0^{-1}\\right]^{-1} \\left[\\Sigma_y^{-1}y + \\Sigma_0^{-1}\\mu_0\\right] \\\\ \\Sigma_1 &= \\left[\\Sigma_y^{-1} + \\Sigma_0^{-1}\\right]^{-1} \\end{aligned} $$ Example for \\(y=\\sum\\theta_i x_i + \\epsilon = X\\theta + \\epsilon \\) with input vector \\(X=(x_1,\\ldots,x_n)\\), measurement noise \\(\\epsilon=\\mathcal{N}(0,\\sigma_y^2)\\) and a priori \\(\\theta\\sim\\mathcal{N}(\\mu_0,\\Sigma_0)\\), $$ p(\\theta|y) \\propto \\mathcal{N}(\\mu_1,\\Sigma_1^2) $$ $$ \\begin{aligned} \\mu_1 &=\\left[\\frac{X^\\top X}{\\sigma_y^2} + \\Sigma_0^{-1}\\right]^{-1} \\left[\\frac{X^\\top y}{\\sigma_y^2} + \\Sigma_0^{-1}\\mu_0\\right] \\\\ \\Sigma_1 &= \\left[\\frac{X^\\top X}{\\sigma_y^2} + \\Sigma_0^{-1}\\right]^{-1} \\end{aligned} $$ $$ \\begin{aligned} p(\\theta|y) &\\propto \\exp\\left(-\\frac{1}{2}\\frac{(y-X\\theta)^2}{\\sigma_y^2}\\right) \\exp\\left(-\\frac{1}{2}(\\theta-\\mu_0)^\\top\\Sigma_0^{-1}(\\theta-\\mu_0)\\right) \\\\ &=\\exp\\left(-\\frac{1}{2}\\left(\\frac{y^2-2yX\\theta+(X\\theta)^2}{\\sigma_y^2}\\theta^\\top\\Sigma_0^{-1}\\theta - 2\\theta^\\top\\Sigma_0^{-1}\\mu_0 + \\mu_0^\\top\\Sigma_0^{-1}\\mu_0\\right)\\right) \\end{aligned} $$ Using \\((X\\theta)^2 = (X\\theta)^\\top X\\theta = \\theta^\\top X^\\top X \\theta\\) and \\(yX\\theta = (yX\\theta)^\\top = \\theta^\\top X^\\top y\\), $$ = \\exp\\left(-\\frac{1}{2}\\left(\\theta^\\top\\left(\\frac{X^\\top X}{\\sigma_y^2}+\\Sigma_0^{-1}\\right)\\theta - 2\\theta^\\top\\left(\\frac{X^\\top y}{\\sigma_y^2}+\\Sigma_0^{-1}\\mu_0\\right)+\\text{cst}\\right)\\right) $$ and we can directly identify the form $$ \\begin{aligned} &\\quad\\exp\\left(-\\frac{1}{2}\\left(\\theta^\\top\\Sigma_1^{-1}\\theta -2\\theta^\\top\\Sigma_1^{-1}\\mu_1 + \\ldots\\right)\\right)\\\\ &\\propto\\exp\\left(-\\frac{1}{2}\\left((\\theta-\\mu_1)^\\top\\Sigma_1^{-1}(\\theta-\\mu_1)\\right)\\right) \\end{aligned} $$ For the multivariate case with \\(y\\in\\mathbb{R}^m\\), \\(X\\in\\mathbb{R}^{m\\times n}\\), \\(\\epsilon\\sim\\mathcal{N}(0,\\Sigma_y)\\), which corresponds to mapping the different lines of \\(X\\) onto elements of \\(y\\) through the same linear transformation with coefficients \\(\\theta\\). If batches of input-output measurements are obtained, we could use this equation. $$ \\begin{aligned} \\mu_1 &=\\left[X^\\top\\Sigma_y^{-1}X + \\Sigma_0^{-1}\\right]^{-1} \\left[X^\\top\\Sigma_y^{-1}y + \\Sigma_0^{-1}\\mu_0\\right] \\\\ \\Sigma_1 &= \\left[X^\\top\\Sigma_y^{-1}X + \\Sigma_0^{-1}\\right]^{-1} \\end{aligned} $$","title":"Recursive Bayesian inference for parameter estimation"},{"location":"Estimation/Kalman/","text":"Kalman filter Model Stochastic linear state-space model, \\begin{aligned} x_{k+1} &= A_d x_k + B_d u_k + Q\\\\ z_k &= H x_k + R \\end{aligned} state is x_k\\sim\\mathcal{N}(m_k,P_k) predicted state is x_k\\sim\\mathcal{N}(m_{k}^\\text{pred},P_{k}^\\text{pred}) output is z_k\\sim\\mathcal{N}(y_k,R) Euler discretization for \\(A_d\\), \\(B_d\\) Using Euler approximation, $$ \\begin{aligned} \\frac{\\Delta x(t)}{\\Delta t} &\\approx Ax(t) + Bu(t) \\\\ \\frac{x_{k+1}-x_k}{\\Delta t} &= Ax_k + Bu_k\\\\ x_{k+1} &= \\underbrace{x_k + \\Delta t A x_k}_{A_d=(I+\\Delta t A)} + \\underbrace{\\Delta t B}_{B_d} u_k \\end{aligned} $$ Equations Prediction \\begin{aligned} m_{k+1}^\\text{pred} &= A_d m_k + B_du_k \\\\ P_{k+1}^\\text{pred} &= A_d P_k A_d^\\top + Q \\end{aligned} Update \\begin{aligned} S_k &= H P_{k}^\\text{pred} H^\\top + R \\\\ K_k &= P_{k}^\\text{pred} H^\\top S_k^{-1} \\\\ m_k &= m_{k}^\\text{pred} + K_k (y_k - H m_{k}^\\text{pred}) \\\\ P_k &= (I - K_k H) P_{k}^\\text{pred} \\end{aligned} A_d , B_d , H can be time-varying.","title":"Kalman filter"},{"location":"Estimation/Kalman/#kalman-filter","text":"","title":"Kalman filter"},{"location":"Estimation/Kalman/#model","text":"Stochastic linear state-space model, \\begin{aligned} x_{k+1} &= A_d x_k + B_d u_k + Q\\\\ z_k &= H x_k + R \\end{aligned} state is x_k\\sim\\mathcal{N}(m_k,P_k) predicted state is x_k\\sim\\mathcal{N}(m_{k}^\\text{pred},P_{k}^\\text{pred}) output is z_k\\sim\\mathcal{N}(y_k,R) Euler discretization for \\(A_d\\), \\(B_d\\) Using Euler approximation, $$ \\begin{aligned} \\frac{\\Delta x(t)}{\\Delta t} &\\approx Ax(t) + Bu(t) \\\\ \\frac{x_{k+1}-x_k}{\\Delta t} &= Ax_k + Bu_k\\\\ x_{k+1} &= \\underbrace{x_k + \\Delta t A x_k}_{A_d=(I+\\Delta t A)} + \\underbrace{\\Delta t B}_{B_d} u_k \\end{aligned} $$","title":"Model"},{"location":"Estimation/Kalman/#equations","text":"Prediction \\begin{aligned} m_{k+1}^\\text{pred} &= A_d m_k + B_du_k \\\\ P_{k+1}^\\text{pred} &= A_d P_k A_d^\\top + Q \\end{aligned} Update \\begin{aligned} S_k &= H P_{k}^\\text{pred} H^\\top + R \\\\ K_k &= P_{k}^\\text{pred} H^\\top S_k^{-1} \\\\ m_k &= m_{k}^\\text{pred} + K_k (y_k - H m_{k}^\\text{pred}) \\\\ P_k &= (I - K_k H) P_{k}^\\text{pred} \\end{aligned} A_d , B_d , H can be time-varying.","title":"Equations"},{"location":"Estimation/gaussian/","text":"Gaussian random variables Definition A Gaussian (or normal ) random variable X has the probability density function (PDF) p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}} Dimension n p(x) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)} It is fully described by two parameters, its mean \\mu and variance \\sigma (or \\Sigma ). We write X\\sim \\mathcal{N}(\\mu,\\sigma^2) , or X\\sim \\mathcal{N}(\\mu,\\Sigma) . The standard normal distribution is \\mathcal{N}(0,1) . The coefficent \\frac{1}{\\sqrt{2\\pi\\sigma^2}} is a normalizing factor so that \\int p(x)dx = 1 . Mean and Variance \\mu = \\mathbb{E}[X] = \\int_\\mathbb{R}xp(x)dx\\\\ \\sigma = \\mathbb{V}[X] = \\mathbb{E}[(X-\\mu)^2] dimension n \\begin{aligned} \\mu &= \\begin{pmatrix} \\mathbb{E}_{x_1}[X] \\\\ \\vdots \\\\ \\mathbb{E}_{x_n}[X] \\end{pmatrix}\\\\ \\Sigma &= \\mathbb{E}\\left[\\left(X-\\mu\\right)\\left(X-\\mu\\right)^\\top\\right] \\end{aligned} Linear transformation Consider the affine transformation Y=AX+b where X\\sim \\mathcal{N}(\\mu_X, \\Sigma_X) . The obtained Y also has a normal distribution with \\begin{aligned} \\mu_Y &= A\\mu_X + b \\\\ \\Sigma_Y &= A\\Sigma_X A^\\top \\end{aligned} Proof for \\(Y=AX+b\\) $$ \\begin{aligned} \\mu_Y &= \\mathbb{E}[AX+b] \\\\ &= A\\mathbb{E}[X] + b \\\\ &= A\\mu_X + b \\\\ \\Sigma_Y &= \\mathbb{E}[(Y-\\bar y)(Y-\\bar y)^\\top] \\\\ &= \\mathbb{E}[(AX+b - A\\mu_X-b)(AX+b - A\\mu_X-b)^\\top] \\\\ &= \\mathbb{E}[A(X-\\mu_X)(X-\\mu_X)^\\top A^\\top] \\\\ &= A\\Sigma_X A^\\top \\end{aligned} $$ Case \\(Y=AX+b+\\epsilon\\) Adding a gaussian noise \\(\\epsilon\\sim\\mathcal{N}(0,\\Sigma_\\epsilon)\\) gives $$ \\begin{aligned} \\mu_Y &= A\\mu_X + b \\\\ \\Sigma_Y &= A\\Sigma_X A^\\top + \\Sigma_\\epsilon \\end{aligned} $$ One particular case is to obtain Y\\sim\\mathcal{N}(\\mu,\\Sigma) from the standard normal variable X\\sim\\mathcal{N}(0,I) , Y = \\mu + \\Sigma^{1/2} X where \\Sigma^{1/2} is the positive semidefinite symmetric matrix such that \\left(\\Sigma^{1/2}\\right)^\\top \\Sigma^{1/2}=\\Sigma . It may be obtained from the eigendecomposition of \\Sigma , in julia that is dist = MvNormal(\u03bc, \u03a3) \u039b,Q = eigen(\u03a3) # \u03a3 = Q\u039b/Q \u03a3sqrt = Q*diagm(sqrt.(\u039b)) Confidence region The minimal volume which contains X with a given probability (ellipsoid for a Gaussian variable). It is more straightforward to compute the probability given a volume, computations are below for dimensions 1, 2, 3, and here 1 for dimension n . It is easier to compute it for a standard normal distribution (a centered segment/circle/sphere), then transform it into an ellipsoid usingthe linear transform for going from \\mathcal{N}(0,I) to \\mathcal{N}(\\mu,\\Sigma) . 1, 2, 3- \\sigma rule for n=1, 2, 3 n 1\\sigma 2\\sigma 3\\sigma 1 68.3 95.5 99.7 2 39.3 86.5 98.9 3 19.9 73.8 97.1 dimension 1: \\(\\mathbb{P}(\\mu - \\sigma \\leq X\\leq \\mu + \\sigma)=0.683\\) Using a change of variable \\(z=\\frac{x-\\mu}{\\sigma}\\), it amounts to computing \\(\\mathbb{P}(-1 \\leq X\\leq 1)\\), $$ \\begin{align} \\mathbb{P}(\\mu - \\sigma \\leq X\\leq \\mu + \\sigma) &= \\int_{\\mu-\\sigma}^{\\mu+\\sigma} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\frac{-1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)}dx \\\\ &= \\int_{-1}^1 \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-x^2}{2}}dx \\\\ &= 0.683 \\end{align} $$ The solution is obtained numerically, there is no analytical solution to this integral. dimension 2: \\(\\mathbb{P}(\\|X\\| \\leq 1)=0.393\\) For a standard normal distribution in dimension 2 \\(X = (X_1, X_2)^\\top\\), $$ \\begin{align} \\mathbb{P}(\\|X\\| \\leq 1) &= \\mathbb{P}(X^\\top X \\leq 1) \\\\ &= \\mathbb{P}(X_1^2 + X_2^2 \\leq 1) \\\\ &= \\int_{x_1^2+x_2^2\\leq 1}\\frac{1}{(2\\pi)^\\frac{n}{2}}e^{-\\frac{x^\\top x}{2}} \\end{align} $$ whith \\(n=2\\) and \\(x=(x_1, x_2)^\\top\\), $$ = \\int_{x_1^2+x_2^2\\leq 1}\\frac{1}{2\\pi}e^{-\\frac{x_1^2+x_2^2}{2}} $$ Change of variable \\(x_1=r\\cos(\\theta)\\), \\(x_2=r\\sin(\\theta)\\) \\(\\implies r=\\sqrt{x_1^2+x_2^2}\\), and first integrating a quantity that does not depend on \\(\\theta\\) from \\(0\\) to \\(2\\pi\\), $$ \\begin{align} &= \\frac{1}{2\\pi}\\int_0^{2\\pi}\\int_0^1 r e^{-\\frac{r^2}{2}} dr d\\theta \\\\ &= \\int_0^1 r e^{-\\frac{r^2}{2}} dr \\\\ &= \\left[ -e^{-\\frac{r^2}{2}} \\right]_0^1 \\\\ &= -e^{-\\frac{1}{2}}-(-e^0) \\\\ &= 1-e^{-\\frac{1}{2}} \\\\ &= 0.393 \\end{align} $$ This is an analytical solution and an inverse formula exists to compute the radius \\(r\\) corresponding to a specific probability \\(p\\), $$ p = 1-e^{-\\frac{r^2}{2}} \\iff r = \\sqrt{-2\\ln(1-p)}. $$ dimension 3: \\(\\mathbb{P}(\\|X\\| \\leq 1)=0.199\\) For a standard normal distribution in dimension 3 \\(X = (X_1, X_2, X_3)^\\top\\), $$ \\begin{align} \\mathbb{P}(\\|X\\| \\leq 1) &= \\mathbb{P}(X^\\top X \\leq 1) \\\\ &= \\mathbb{P}(X_1^2 + X_2^2 + X_3^2\\leq 1) \\\\ &= \\int_{x_1^2+x_2^2+x_3^2\\leq 1}\\frac{1}{(2\\pi)^\\frac{n}{2}}e^{-\\frac{x^\\top x}{2}} \\end{align} $$ whith \\(n=3\\) and \\(x=(x_1, x_2, x_3)^\\top\\), $$ = \\int_{x_1^2+x_2^2+x_3^2\\leq 1}\\frac{1}{(2\\pi)^{3/2}}e^{-\\frac{x_1^2+x_2^2+x_3^2}{2}} $$ Change of variable \\(x_1=r\\sin(\\phi)\\cos(\\theta)\\), \\(x_2=r\\sin(\\phi)\\sin(\\theta)\\), \\(x_3=r\\cos(\\phi)\\) \\(\\implies r=\\sqrt{x_1^2+x_2^2+x_3^2}\\), and first integrating a quantity that does not depend on \\(\\theta\\) from \\(0\\) to \\(2\\pi\\), and then \\(\\sin(\\phi)\\) from \\(0\\) to \\(\\pi\\) which gives \\(-\\cos\\pi - (-\\cos 0) = 2\\), $$ \\begin{align} &= \\frac{1}{2\\pi^{3/2}}\\int_0^{2\\pi}\\int_0^\\pi\\int_0^1 r^2\\sin(\\phi)e^{-\\frac{r^2}{2}} dr d\\phi d\\theta \\\\ &= \\frac{2\\pi}{2\\pi^{3/2}}\\int_0^\\pi\\sin(\\phi)\\int_0^1 r^2e^{-\\frac{r^2}{2}} dr d\\phi \\\\ &= \\frac{2}{\\sqrt{2\\pi}}\\int_0^1 r^2e^{-\\frac{r^2}{2}} dr \\\\ &= 0.199 \\end{align} $$ The solution is obtained numerically, there is no analytical solution to this integral. Wang, B., Shi, W., & Miao, Z. (2015). Confidence analysis of standard deviational ellipse and its extension into higher dimensional Euclidean space. PloS one, 10(3), e0118537. \u21a9","title":"Gaussian"},{"location":"Estimation/gaussian/#gaussian-random-variables","text":"","title":"Gaussian random variables"},{"location":"Estimation/gaussian/#definition","text":"A Gaussian (or normal ) random variable X has the probability density function (PDF) p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}} Dimension n p(x) = \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^\\top \\Sigma ^{-1}(x-\\mu)} It is fully described by two parameters, its mean \\mu and variance \\sigma (or \\Sigma ). We write X\\sim \\mathcal{N}(\\mu,\\sigma^2) , or X\\sim \\mathcal{N}(\\mu,\\Sigma) . The standard normal distribution is \\mathcal{N}(0,1) . The coefficent \\frac{1}{\\sqrt{2\\pi\\sigma^2}} is a normalizing factor so that \\int p(x)dx = 1 .","title":"Definition"},{"location":"Estimation/gaussian/#mean-and-variance","text":"\\mu = \\mathbb{E}[X] = \\int_\\mathbb{R}xp(x)dx\\\\ \\sigma = \\mathbb{V}[X] = \\mathbb{E}[(X-\\mu)^2] dimension n \\begin{aligned} \\mu &= \\begin{pmatrix} \\mathbb{E}_{x_1}[X] \\\\ \\vdots \\\\ \\mathbb{E}_{x_n}[X] \\end{pmatrix}\\\\ \\Sigma &= \\mathbb{E}\\left[\\left(X-\\mu\\right)\\left(X-\\mu\\right)^\\top\\right] \\end{aligned}","title":"Mean and Variance"},{"location":"Estimation/gaussian/#linear-transformation","text":"Consider the affine transformation Y=AX+b where X\\sim \\mathcal{N}(\\mu_X, \\Sigma_X) . The obtained Y also has a normal distribution with \\begin{aligned} \\mu_Y &= A\\mu_X + b \\\\ \\Sigma_Y &= A\\Sigma_X A^\\top \\end{aligned} Proof for \\(Y=AX+b\\) $$ \\begin{aligned} \\mu_Y &= \\mathbb{E}[AX+b] \\\\ &= A\\mathbb{E}[X] + b \\\\ &= A\\mu_X + b \\\\ \\Sigma_Y &= \\mathbb{E}[(Y-\\bar y)(Y-\\bar y)^\\top] \\\\ &= \\mathbb{E}[(AX+b - A\\mu_X-b)(AX+b - A\\mu_X-b)^\\top] \\\\ &= \\mathbb{E}[A(X-\\mu_X)(X-\\mu_X)^\\top A^\\top] \\\\ &= A\\Sigma_X A^\\top \\end{aligned} $$ Case \\(Y=AX+b+\\epsilon\\) Adding a gaussian noise \\(\\epsilon\\sim\\mathcal{N}(0,\\Sigma_\\epsilon)\\) gives $$ \\begin{aligned} \\mu_Y &= A\\mu_X + b \\\\ \\Sigma_Y &= A\\Sigma_X A^\\top + \\Sigma_\\epsilon \\end{aligned} $$ One particular case is to obtain Y\\sim\\mathcal{N}(\\mu,\\Sigma) from the standard normal variable X\\sim\\mathcal{N}(0,I) , Y = \\mu + \\Sigma^{1/2} X where \\Sigma^{1/2} is the positive semidefinite symmetric matrix such that \\left(\\Sigma^{1/2}\\right)^\\top \\Sigma^{1/2}=\\Sigma . It may be obtained from the eigendecomposition of \\Sigma , in julia that is dist = MvNormal(\u03bc, \u03a3) \u039b,Q = eigen(\u03a3) # \u03a3 = Q\u039b/Q \u03a3sqrt = Q*diagm(sqrt.(\u039b))","title":"Linear transformation"},{"location":"Estimation/gaussian/#confidence-region","text":"The minimal volume which contains X with a given probability (ellipsoid for a Gaussian variable). It is more straightforward to compute the probability given a volume, computations are below for dimensions 1, 2, 3, and here 1 for dimension n . It is easier to compute it for a standard normal distribution (a centered segment/circle/sphere), then transform it into an ellipsoid usingthe linear transform for going from \\mathcal{N}(0,I) to \\mathcal{N}(\\mu,\\Sigma) . 1, 2, 3- \\sigma rule for n=1, 2, 3 n 1\\sigma 2\\sigma 3\\sigma 1 68.3 95.5 99.7 2 39.3 86.5 98.9 3 19.9 73.8 97.1 dimension 1: \\(\\mathbb{P}(\\mu - \\sigma \\leq X\\leq \\mu + \\sigma)=0.683\\) Using a change of variable \\(z=\\frac{x-\\mu}{\\sigma}\\), it amounts to computing \\(\\mathbb{P}(-1 \\leq X\\leq 1)\\), $$ \\begin{align} \\mathbb{P}(\\mu - \\sigma \\leq X\\leq \\mu + \\sigma) &= \\int_{\\mu-\\sigma}^{\\mu+\\sigma} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{\\frac{-1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)}dx \\\\ &= \\int_{-1}^1 \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-x^2}{2}}dx \\\\ &= 0.683 \\end{align} $$ The solution is obtained numerically, there is no analytical solution to this integral. dimension 2: \\(\\mathbb{P}(\\|X\\| \\leq 1)=0.393\\) For a standard normal distribution in dimension 2 \\(X = (X_1, X_2)^\\top\\), $$ \\begin{align} \\mathbb{P}(\\|X\\| \\leq 1) &= \\mathbb{P}(X^\\top X \\leq 1) \\\\ &= \\mathbb{P}(X_1^2 + X_2^2 \\leq 1) \\\\ &= \\int_{x_1^2+x_2^2\\leq 1}\\frac{1}{(2\\pi)^\\frac{n}{2}}e^{-\\frac{x^\\top x}{2}} \\end{align} $$ whith \\(n=2\\) and \\(x=(x_1, x_2)^\\top\\), $$ = \\int_{x_1^2+x_2^2\\leq 1}\\frac{1}{2\\pi}e^{-\\frac{x_1^2+x_2^2}{2}} $$ Change of variable \\(x_1=r\\cos(\\theta)\\), \\(x_2=r\\sin(\\theta)\\) \\(\\implies r=\\sqrt{x_1^2+x_2^2}\\), and first integrating a quantity that does not depend on \\(\\theta\\) from \\(0\\) to \\(2\\pi\\), $$ \\begin{align} &= \\frac{1}{2\\pi}\\int_0^{2\\pi}\\int_0^1 r e^{-\\frac{r^2}{2}} dr d\\theta \\\\ &= \\int_0^1 r e^{-\\frac{r^2}{2}} dr \\\\ &= \\left[ -e^{-\\frac{r^2}{2}} \\right]_0^1 \\\\ &= -e^{-\\frac{1}{2}}-(-e^0) \\\\ &= 1-e^{-\\frac{1}{2}} \\\\ &= 0.393 \\end{align} $$ This is an analytical solution and an inverse formula exists to compute the radius \\(r\\) corresponding to a specific probability \\(p\\), $$ p = 1-e^{-\\frac{r^2}{2}} \\iff r = \\sqrt{-2\\ln(1-p)}. $$ dimension 3: \\(\\mathbb{P}(\\|X\\| \\leq 1)=0.199\\) For a standard normal distribution in dimension 3 \\(X = (X_1, X_2, X_3)^\\top\\), $$ \\begin{align} \\mathbb{P}(\\|X\\| \\leq 1) &= \\mathbb{P}(X^\\top X \\leq 1) \\\\ &= \\mathbb{P}(X_1^2 + X_2^2 + X_3^2\\leq 1) \\\\ &= \\int_{x_1^2+x_2^2+x_3^2\\leq 1}\\frac{1}{(2\\pi)^\\frac{n}{2}}e^{-\\frac{x^\\top x}{2}} \\end{align} $$ whith \\(n=3\\) and \\(x=(x_1, x_2, x_3)^\\top\\), $$ = \\int_{x_1^2+x_2^2+x_3^2\\leq 1}\\frac{1}{(2\\pi)^{3/2}}e^{-\\frac{x_1^2+x_2^2+x_3^2}{2}} $$ Change of variable \\(x_1=r\\sin(\\phi)\\cos(\\theta)\\), \\(x_2=r\\sin(\\phi)\\sin(\\theta)\\), \\(x_3=r\\cos(\\phi)\\) \\(\\implies r=\\sqrt{x_1^2+x_2^2+x_3^2}\\), and first integrating a quantity that does not depend on \\(\\theta\\) from \\(0\\) to \\(2\\pi\\), and then \\(\\sin(\\phi)\\) from \\(0\\) to \\(\\pi\\) which gives \\(-\\cos\\pi - (-\\cos 0) = 2\\), $$ \\begin{align} &= \\frac{1}{2\\pi^{3/2}}\\int_0^{2\\pi}\\int_0^\\pi\\int_0^1 r^2\\sin(\\phi)e^{-\\frac{r^2}{2}} dr d\\phi d\\theta \\\\ &= \\frac{2\\pi}{2\\pi^{3/2}}\\int_0^\\pi\\sin(\\phi)\\int_0^1 r^2e^{-\\frac{r^2}{2}} dr d\\phi \\\\ &= \\frac{2}{\\sqrt{2\\pi}}\\int_0^1 r^2e^{-\\frac{r^2}{2}} dr \\\\ &= 0.199 \\end{align} $$ The solution is obtained numerically, there is no analytical solution to this integral. Wang, B., Shi, W., & Miao, Z. (2015). Confidence analysis of standard deviational ellipse and its extension into higher dimensional Euclidean space. PloS one, 10(3), e0118537. \u21a9","title":"Confidence region"},{"location":"Geometry2D/circle_circle_intersection/","text":"Circle-circle intersection Obtained from math.stackexchange.com/questions/256100/ . Solution Intersection points of a circle of radius r_1 centered on (x_1,y_1) with another circle of radius r_2 centered on (x_2,y_2) . The two intersection points are P_i = \\begin{pmatrix} \\frac{x_1+x_2}{2}\\\\ \\frac{y_1+y_2}{2} \\end{pmatrix} +\\frac{r_1^2-r_2^2}{2R} \\begin{pmatrix} \\frac{x_1+x_2}{R}\\\\ \\frac{y_1+y_2}{R} \\end{pmatrix} \\pm \\sqrt{\\frac{r_1^2+r_2^2}{2} -\\left(\\frac{r_1^2-r_2^2}{4R}\\right)^2-\\frac{R^2}{4}} \\begin{pmatrix} \\frac{y_2+y_1}{R}\\\\ \\frac{x_1+x_1}{R} \\end{pmatrix} Derivation Get to an easier frame for solving Compute the middle point C_m = \\begin{pmatrix} \\frac{x_1+x_2}{2}\\\\ \\frac{y_1+y_2}{2} \\end{pmatrix} and the distance between the two centers R = \\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2} Now consider the coordinate frame with origin at C_m and unit axes \\vec{a} = \\begin{pmatrix} \\frac{x_1+x_2}{R}\\\\ \\frac{y_1+y_2}{R} \\end{pmatrix} and \\vec{b} = \\begin{pmatrix} \\frac{y_2+y_1}{R}\\\\ \\frac{x_1+x_1}{R} \\end{pmatrix} Equation of circles in new coordinate frame Usual equation of a circle is (x-x_1)^2 + (y-y_1)^2 = r_1^2 , which is obtain from the Pythagorean theorem with r_1 the hypothenuse and (x-x_1) and (y-y_1) its projection on \\vec{x} and \\vec{y} axes. Now in our case, the circles are centered along the dashed line so their centers have no \\vec{b} component. Equations are (a+R/2)^2 + b^2 = r_1^2\\\\ (a-R/2)^2 + b^2 = r_2^2 Equating the two circles equation, the intersection points are obtained in the (\\vec{a},\\vec{b}) frame, a =\\frac{r_1^2-r_2^2}{2R}\\\\ b = \\sqrt{\\frac{r_1^2+r_2^2}{2} -\\left(\\frac{r_1^2-r_2^2}{4R}\\right)^2-\\frac{R^2}{4}} The two intersection points are thus P_i = C_m + a \\vec{a} \\pm b \\vec{b}\\\\ P_i =\\begin{pmatrix} \\frac{x_1+x_2}{2}\\\\ \\frac{y_1+y_2}{2} \\end{pmatrix} +a \\begin{pmatrix} \\frac{x_1+x_2}{R}\\\\ \\frac{y_1+y_2}{R} \\end{pmatrix} \\pm b \\begin{pmatrix} \\frac{y_2+y_1}{R}\\\\ \\frac{x_1+x_1}{R} \\end{pmatrix}\\\\ P_i = \\begin{pmatrix} \\frac{x_1+x_2}{2}\\\\ \\frac{y_1+y_2}{2} \\end{pmatrix} +\\frac{r_1^2-r_2^2}{2R} \\begin{pmatrix} \\frac{x_1+x_2}{R}\\\\ \\frac{y_1+y_2}{R} \\end{pmatrix} \\pm \\sqrt{\\frac{r_1^2+r_2^2}{2} -\\left(\\frac{r_1^2-r_2^2}{4R}\\right)^2-\\frac{R^2}{4}} \\begin{pmatrix} \\frac{y_2+y_1}{R}\\\\ \\frac{x_1+x_1}{R} \\end{pmatrix} Check for existence The intersection only exists if R<r_1 + r_2 and R>|r_2-r_1| (with |\\cdot | the notation for absolute value). First, \\(a\\) is obstained by subtracting the second to the first equation, $$(a+R/2)^2 - (a-R/2)^2 = r_1^2 - r_2^2\\\\ a^2 + a R + (R/2)^2 - \\left(a^2 - aR + (R/2)^2\\right) = r_1^2-r_2^2\\\\ a =\\frac{r_1^2-r_2^2}{2R}$$ Then \\(b\\) is obtained by adding the first and second equations, $$2b^2 = r_1^2+r_2^2 - (a+R/2)^2 - (a-R/2)^2\\\\ 2b^2 = r_1^2+r_2^2 -\\left(\\frac{r_1^2-r_2^2}{2R}\\right)^2-\\frac{r_1^2-r_2^2}{2} - R^2/4\\\\ - \\left(\\frac{r_1^2-r_2^2}{2R}\\right)^2+\\frac{r_1^2-r_2^2}{2} - R^2/4\\\\ 2b^2 = r_1^2+r_2^2 -2\\left(\\frac{r_1^2-r_2^2}{2R}\\right)^2 - 2R^2/4\\\\ b = \\sqrt{\\frac{r_1^2+r_2^2}{2} -\\left(\\frac{r_1^2-r_2^2}{4R}\\right)^2-\\frac{R^2}{4}}$$","title":"Geometry2D circle-circle intersection"},{"location":"Geometry2D/circle_circle_intersection/#circle-circle-intersection","text":"Obtained from math.stackexchange.com/questions/256100/ .","title":"Circle-circle intersection"},{"location":"Geometry2D/circle_circle_intersection/#solution","text":"Intersection points of a circle of radius r_1 centered on (x_1,y_1) with another circle of radius r_2 centered on (x_2,y_2) . The two intersection points are P_i = \\begin{pmatrix} \\frac{x_1+x_2}{2}\\\\ \\frac{y_1+y_2}{2} \\end{pmatrix} +\\frac{r_1^2-r_2^2}{2R} \\begin{pmatrix} \\frac{x_1+x_2}{R}\\\\ \\frac{y_1+y_2}{R} \\end{pmatrix} \\pm \\sqrt{\\frac{r_1^2+r_2^2}{2} -\\left(\\frac{r_1^2-r_2^2}{4R}\\right)^2-\\frac{R^2}{4}} \\begin{pmatrix} \\frac{y_2+y_1}{R}\\\\ \\frac{x_1+x_1}{R} \\end{pmatrix}","title":"Solution"},{"location":"Geometry2D/circle_circle_intersection/#derivation","text":"","title":"Derivation"},{"location":"Geometry2D/circle_circle_intersection/#get-to-an-easier-frame-for-solving","text":"Compute the middle point C_m = \\begin{pmatrix} \\frac{x_1+x_2}{2}\\\\ \\frac{y_1+y_2}{2} \\end{pmatrix} and the distance between the two centers R = \\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2} Now consider the coordinate frame with origin at C_m and unit axes \\vec{a} = \\begin{pmatrix} \\frac{x_1+x_2}{R}\\\\ \\frac{y_1+y_2}{R} \\end{pmatrix} and \\vec{b} = \\begin{pmatrix} \\frac{y_2+y_1}{R}\\\\ \\frac{x_1+x_1}{R} \\end{pmatrix}","title":"Get to an easier frame for solving"},{"location":"Geometry2D/circle_circle_intersection/#equation-of-circles-in-new-coordinate-frame","text":"Usual equation of a circle is (x-x_1)^2 + (y-y_1)^2 = r_1^2 , which is obtain from the Pythagorean theorem with r_1 the hypothenuse and (x-x_1) and (y-y_1) its projection on \\vec{x} and \\vec{y} axes. Now in our case, the circles are centered along the dashed line so their centers have no \\vec{b} component. Equations are (a+R/2)^2 + b^2 = r_1^2\\\\ (a-R/2)^2 + b^2 = r_2^2 Equating the two circles equation, the intersection points are obtained in the (\\vec{a},\\vec{b}) frame, a =\\frac{r_1^2-r_2^2}{2R}\\\\ b = \\sqrt{\\frac{r_1^2+r_2^2}{2} -\\left(\\frac{r_1^2-r_2^2}{4R}\\right)^2-\\frac{R^2}{4}} The two intersection points are thus P_i = C_m + a \\vec{a} \\pm b \\vec{b}\\\\ P_i =\\begin{pmatrix} \\frac{x_1+x_2}{2}\\\\ \\frac{y_1+y_2}{2} \\end{pmatrix} +a \\begin{pmatrix} \\frac{x_1+x_2}{R}\\\\ \\frac{y_1+y_2}{R} \\end{pmatrix} \\pm b \\begin{pmatrix} \\frac{y_2+y_1}{R}\\\\ \\frac{x_1+x_1}{R} \\end{pmatrix}\\\\ P_i = \\begin{pmatrix} \\frac{x_1+x_2}{2}\\\\ \\frac{y_1+y_2}{2} \\end{pmatrix} +\\frac{r_1^2-r_2^2}{2R} \\begin{pmatrix} \\frac{x_1+x_2}{R}\\\\ \\frac{y_1+y_2}{R} \\end{pmatrix} \\pm \\sqrt{\\frac{r_1^2+r_2^2}{2} -\\left(\\frac{r_1^2-r_2^2}{4R}\\right)^2-\\frac{R^2}{4}} \\begin{pmatrix} \\frac{y_2+y_1}{R}\\\\ \\frac{x_1+x_1}{R} \\end{pmatrix}","title":"Equation of circles in new coordinate frame"},{"location":"Geometry2D/circle_circle_intersection/#check-for-existence","text":"The intersection only exists if R<r_1 + r_2 and R>|r_2-r_1| (with |\\cdot | the notation for absolute value). First, \\(a\\) is obstained by subtracting the second to the first equation, $$(a+R/2)^2 - (a-R/2)^2 = r_1^2 - r_2^2\\\\ a^2 + a R + (R/2)^2 - \\left(a^2 - aR + (R/2)^2\\right) = r_1^2-r_2^2\\\\ a =\\frac{r_1^2-r_2^2}{2R}$$ Then \\(b\\) is obtained by adding the first and second equations, $$2b^2 = r_1^2+r_2^2 - (a+R/2)^2 - (a-R/2)^2\\\\ 2b^2 = r_1^2+r_2^2 -\\left(\\frac{r_1^2-r_2^2}{2R}\\right)^2-\\frac{r_1^2-r_2^2}{2} - R^2/4\\\\ - \\left(\\frac{r_1^2-r_2^2}{2R}\\right)^2+\\frac{r_1^2-r_2^2}{2} - R^2/4\\\\ 2b^2 = r_1^2+r_2^2 -2\\left(\\frac{r_1^2-r_2^2}{2R}\\right)^2 - 2R^2/4\\\\ b = \\sqrt{\\frac{r_1^2+r_2^2}{2} -\\left(\\frac{r_1^2-r_2^2}{4R}\\right)^2-\\frac{R^2}{4}}$$","title":"Check for existence"},{"location":"Tools/markdown/","text":"Markdown Check out mkdocs-material for many guides and plugins. highest title style has two possible synthax, # Title or Titles ====== Second-highest title style has two possible synthax, ## H2 or H2 ------ H3 Other titles are obtained as ### H3 , #### H4 , ##### H5 , ###### H6 . H4 H5 H6 Use at least three - , * or _ , e.g. --- for a horizontal rule. Text style Comment markdown code as <!-- This content will not appear in the rendered Markdown --> Leave a blank line for a new paragraph with space in-between. Leave two spaces at end of line for line break, or use \\ . **bold text** for bold text _italicized text_ for italicized text ~~strikethrough text~~ for ~~strikethrough text~~ _italic_ inside **bold text** for bold italic text ***bold italic text*** for bold italic text <sup>superscript</sup> text for superscript text <ins>underlined</ins> text for underlined text `code` for code > quote for a quote second line in quote Internet links [GitHub link](https://github.com) for GitHub link Local links [Link to code section](#code) for Link to code section in other file ![reference to local file](img/classmate.png) for a reference to local file ![two people](img/classmate.png) for Using the attr_list extension, we can use markdown synthax and parameterize html elements, ![two people](img/classmate.png){: style=\"display: block; margin: 0 auto; width: 300px\"} for a centered image with specific width (height is automatically adjusted to keep ratio) Code Blocks of code for specific languages are written by encompassing them with ``` and specifying the language on the first ``` , e.g. ```cpp . #include<iostream> int main() { std::cout << \"Hello World!\\n\"; } Lists Unordered lists with either - , * or + at begining of line, and ordered lists with 1. , 2. , etc. Nested lists with a tab, so marker is below above item's text. - item a 1. item a1 2. item a2 * item b * some item * some other item - with a sub-item + item c item a item a1 item a2 item b some item some other item with a sub-item item c Tables Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 Equations For inline math, use \\( a \\) for a For equations, use $$ $$ The Cauchy-Schwarz Inequality, \\left( \\sum_{k=1}^n a_k b_k \\right)^2 \\leq \\left( \\sum_{k=1}^n a_k^2 \\right) \\left( \\sum_{k=1}^n b_k^2 \\right) Using extension mdx_math, aligned environment can be used, \\begin{align} x &= y + z + j^{-1} \\\\ a &= b + c \\end{align} blocks Using markdown extension admonition !!! note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod You can use note , abstract , info , tip , success , question , warning , failure , danger , bug , example , quote . The title is optional. Footnotes use [^1] for a footnote 1 , and another 2 . Then add [^1]: explanation anywhere in the doc, it will be rendered at the bottom. For a paragraph, indent it with four spaces, e.g. [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Collapsible sections Tips for collapsed sections ### You can add a header You can add text within a collapsed section. You can add an image or a code block, too. puts \"Hello World\" Collapsed sections This one is opened by default Section A Section A.B Section A.B.C Section A.B.C.D Done! explanation \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Mardown"},{"location":"Tools/markdown/#markdown","text":"Check out mkdocs-material for many guides and plugins. highest title style has two possible synthax, # Title or Titles ====== Second-highest title style has two possible synthax, ## H2 or H2 ------","title":"Markdown"},{"location":"Tools/markdown/#h3","text":"Other titles are obtained as ### H3 , #### H4 , ##### H5 , ###### H6 .","title":"H3"},{"location":"Tools/markdown/#h4","text":"","title":"H4"},{"location":"Tools/markdown/#h5","text":"","title":"H5"},{"location":"Tools/markdown/#h6","text":"Use at least three - , * or _ , e.g. --- for a horizontal rule.","title":"H6"},{"location":"Tools/markdown/#text-style","text":"Comment markdown code as <!-- This content will not appear in the rendered Markdown --> Leave a blank line for a new paragraph with space in-between. Leave two spaces at end of line for line break, or use \\ . **bold text** for bold text _italicized text_ for italicized text ~~strikethrough text~~ for ~~strikethrough text~~ _italic_ inside **bold text** for bold italic text ***bold italic text*** for bold italic text <sup>superscript</sup> text for superscript text <ins>underlined</ins> text for underlined text `code` for code > quote for a quote second line in quote","title":"Text style"},{"location":"Tools/markdown/#internet-links","text":"[GitHub link](https://github.com) for GitHub link","title":"Internet links"},{"location":"Tools/markdown/#local-links","text":"[Link to code section](#code) for Link to code section in other file ![reference to local file](img/classmate.png) for a reference to local file ![two people](img/classmate.png) for Using the attr_list extension, we can use markdown synthax and parameterize html elements, ![two people](img/classmate.png){: style=\"display: block; margin: 0 auto; width: 300px\"} for a centered image with specific width (height is automatically adjusted to keep ratio)","title":"Local links"},{"location":"Tools/markdown/#code","text":"Blocks of code for specific languages are written by encompassing them with ``` and specifying the language on the first ``` , e.g. ```cpp . #include<iostream> int main() { std::cout << \"Hello World!\\n\"; }","title":"Code"},{"location":"Tools/markdown/#lists","text":"Unordered lists with either - , * or + at begining of line, and ordered lists with 1. , 2. , etc. Nested lists with a tab, so marker is below above item's text. - item a 1. item a1 2. item a2 * item b * some item * some other item - with a sub-item + item c item a item a1 item a2 item b some item some other item with a sub-item item c","title":"Lists"},{"location":"Tools/markdown/#tables","text":"Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1","title":"Tables"},{"location":"Tools/markdown/#equations","text":"For inline math, use \\( a \\) for a For equations, use $$ $$ The Cauchy-Schwarz Inequality, \\left( \\sum_{k=1}^n a_k b_k \\right)^2 \\leq \\left( \\sum_{k=1}^n a_k^2 \\right) \\left( \\sum_{k=1}^n b_k^2 \\right) Using extension mdx_math, aligned environment can be used, \\begin{align} x &= y + z + j^{-1} \\\\ a &= b + c \\end{align}","title":"Equations"},{"location":"Tools/markdown/#blocks","text":"Using markdown extension admonition !!! note \"Phasellus posuere in sem ut cursus\" Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod You can use note , abstract , info , tip , success , question , warning , failure , danger , bug , example , quote . The title is optional.","title":"blocks"},{"location":"Tools/markdown/#footnotes","text":"use [^1] for a footnote 1 , and another 2 . Then add [^1]: explanation anywhere in the doc, it will be rendered at the bottom. For a paragraph, indent it with four spaces, e.g. [^2]: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Footnotes"},{"location":"Tools/markdown/#collapsible-sections","text":"Tips for collapsed sections ### You can add a header You can add text within a collapsed section. You can add an image or a code block, too. puts \"Hello World\" Collapsed sections This one is opened by default Section A Section A.B Section A.B.C Section A.B.C.D Done! explanation \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9","title":"Collapsible sections"},{"location":"Tools/mkdocs/","text":"MkDocs Deployment of MkDocs site on GitHub Pages Install MkDocs Create a new github repository In the main branch, create a new mkdocs project Run it locally mkdocs serve , check it out at http://127.0.0.1:8000/ In mkdocs.yml file, ensure site_url: https://your-username.github.io/your-repo-name/ In GitHub > Settings > Pages, set Source to \"deploy from a branch\" select branch gh-deploy at /(root) Deploy using mkdocs gh-deploy --clean The site is accessible after a few minutes!","title":"MkDocs"},{"location":"Tools/mkdocs/#mkdocs","text":"","title":"MkDocs"},{"location":"Tools/mkdocs/#deployment-of-mkdocs-site-on-github-pages","text":"Install MkDocs Create a new github repository In the main branch, create a new mkdocs project Run it locally mkdocs serve , check it out at http://127.0.0.1:8000/ In mkdocs.yml file, ensure site_url: https://your-username.github.io/your-repo-name/ In GitHub > Settings > Pages, set Source to \"deploy from a branch\" select branch gh-deploy at /(root) Deploy using mkdocs gh-deploy --clean The site is accessible after a few minutes!","title":"Deployment of MkDocs site on GitHub Pages"},{"location":"Tools/ros2/","text":"ROS2 source /opt/ros/jazzy/setup.bash source <install_folder>/local_setup.bash rqt # gui to replace many command-line colcon_cd <pkg_name> # cd to pkg folder Build # resolve dependencies rosdep install -i --from-path src --rosdistro jazzy -y # build colcon build --symlink-install # build (python modifications do not require rebuild) colcon build --packages-select <pkg_name> # rebuild only specific package colcon build --event-handlers console_direct+ # show console output to debug To not build a specific package, place an empty file COLCON_IGNORE in its directory. Introspection ros2 run rqt_graph rqt_graph ros2 run rqt_console rqt_console # to inspect logs ros2 pkg executables <pkg_name> ros2 node list ros2 topic list -t # -t to show topic type ros2 service list ros2 action list ros2 node info <node_name> ros2 topic info <topic_name> # show type, publishers, subscribers ros2 interface show <msg_name> # show details of message structure ros2 topic hz <topic_name> ros2 topic bw <topic_name> # bandwith (data transfer rate in KB/s and msg size in KB) ros2 topic echo <topic_name> Operations ros2 launch <pkg_name> <launch_name> # launch files have extension .launch.py ros2 run <pkg_name> <exec_name> Parameters ros2 param get <node_name> <param_name> ros2 param set <node_name> <param_name> <value> ros2 param dump <node name> # print all param and values to stdout ros2 param dump /turtlesim > cfg1.yaml # print to file ros2 param load /turtlesim cfg1.yaml # load parameters ros2 run <pkg_name> <exec_name> --ros-args --params-file cfg1.yaml # run with param Bags cd /bag_files # move to where bag should be saved ros2 bag record -o my_bag_name <topic_name1> <topic_name2> ros2 bag info <bag_file> ros2 bag play --clock --loop <bag_file>","title":"command-line"},{"location":"Tools/ros2/#ros2","text":"source /opt/ros/jazzy/setup.bash source <install_folder>/local_setup.bash rqt # gui to replace many command-line colcon_cd <pkg_name> # cd to pkg folder","title":"ROS2"},{"location":"Tools/ros2/#build","text":"# resolve dependencies rosdep install -i --from-path src --rosdistro jazzy -y # build colcon build --symlink-install # build (python modifications do not require rebuild) colcon build --packages-select <pkg_name> # rebuild only specific package colcon build --event-handlers console_direct+ # show console output to debug To not build a specific package, place an empty file COLCON_IGNORE in its directory.","title":"Build"},{"location":"Tools/ros2/#introspection","text":"ros2 run rqt_graph rqt_graph ros2 run rqt_console rqt_console # to inspect logs ros2 pkg executables <pkg_name> ros2 node list ros2 topic list -t # -t to show topic type ros2 service list ros2 action list ros2 node info <node_name> ros2 topic info <topic_name> # show type, publishers, subscribers ros2 interface show <msg_name> # show details of message structure ros2 topic hz <topic_name> ros2 topic bw <topic_name> # bandwith (data transfer rate in KB/s and msg size in KB) ros2 topic echo <topic_name>","title":"Introspection"},{"location":"Tools/ros2/#operations","text":"ros2 launch <pkg_name> <launch_name> # launch files have extension .launch.py ros2 run <pkg_name> <exec_name>","title":"Operations"},{"location":"Tools/ros2/#parameters","text":"ros2 param get <node_name> <param_name> ros2 param set <node_name> <param_name> <value> ros2 param dump <node name> # print all param and values to stdout ros2 param dump /turtlesim > cfg1.yaml # print to file ros2 param load /turtlesim cfg1.yaml # load parameters ros2 run <pkg_name> <exec_name> --ros-args --params-file cfg1.yaml # run with param","title":"Parameters"},{"location":"Tools/ros2/#bags","text":"cd /bag_files # move to where bag should be saved ros2 bag record -o my_bag_name <topic_name1> <topic_name2> ros2 bag info <bag_file> ros2 bag play --clock --loop <bag_file>","title":"Bags"},{"location":"Tools/ros2_cpp/","text":"ROS2 C++ package Structure my_package/ config/ # yaml config files rviz/ # rviz config files doc/ # documentation msg/ # custom msg definitions launch/ # launch files include/my_package # headers .h and .hpp src/ # C++ source code urdf/ srv/ action/ test/ CMakeLists.txt # instructions to build code package.xml # meta info LICENSE Creation ros2 pkg create --build-type ament_cmake --license Apache-2.0 --node-name my_node my_package","title":"C++ packages"},{"location":"Tools/ros2_cpp/#ros2-c-package","text":"Structure my_package/ config/ # yaml config files rviz/ # rviz config files doc/ # documentation msg/ # custom msg definitions launch/ # launch files include/my_package # headers .h and .hpp src/ # C++ source code urdf/ srv/ action/ test/ CMakeLists.txt # instructions to build code package.xml # meta info LICENSE Creation ros2 pkg create --build-type ament_cmake --license Apache-2.0 --node-name my_node my_package","title":"ROS2 C++ package"},{"location":"Tools/ros2_python/","text":"ROS2 Python package Structure my_package/ package.xml # meta info resource/my_package # marker file setup.cfg # required for executables for ros2 run, must match with meta info setup.py # instructions to install pkg my_package/ # required to find pkg, contains __init__.py Creation ros2 pkg create --build-type ament_python --license Apache-2.0 --node-name my_node my_package","title":"Python packages"},{"location":"Tools/ros2_python/#ros2-python-package","text":"Structure my_package/ package.xml # meta info resource/my_package # marker file setup.cfg # required for executables for ros2 run, must match with meta info setup.py # instructions to install pkg my_package/ # required to find pkg, contains __init__.py Creation ros2 pkg create --build-type ament_python --license Apache-2.0 --node-name my_node my_package","title":"ROS2 Python package"}]}